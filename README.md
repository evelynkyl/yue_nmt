# Extremely Low-Resource Neural Machine Translation: A Case Study of Cantonese

This repo provides the implentation scripts in the project, as well as the synthetic data generated via bitext mining.

The development of NLP applications for Cantonese, a language with over 85 million speakers, is lagging compared to other languages with a similar number of speakers. In this paper, we present, to our best knowledge, the first benchmark of multiple neural machine translation (NMT) systems between Cantonese and Mandarin Chinese. Secondly, we performed parallel sentence mining as data augmentation for the extremely low resource language pair and increased the number of sentence pairs by 3480% (1,002 to 35,877). Results show that with the parallel sentence mining technique, the best performing model (BPE-level bidirectional LSTM) scored 11.98 BLEU better than the vanilla baseline and 9.93 BLEU higher than our strong baseline. Thirdly, we evaluated the quality of the translated texts using modern texts and historical texts to investigate the models' ability to translate historical texts. Finally, we provide the first large-scale parallel training dataset of the language pair (post-sentence mining) as well as an evaluation dataset comprised of Cantonese, Mandarin, and Literary Chinese for future research.
