{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install VS code environment ","metadata":{}},{"cell_type":"code","source":"!pip install colabcode","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from colabcode import ColabCode\n\nColabCode(port=10000, password=\"per4288\")","metadata":{"execution":{"iopub.status.busy":"2021-11-06T11:04:39.335570Z","iopub.execute_input":"2021-11-06T11:04:39.335866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downloading and Extracting data sets","metadata":{"id":"vAF2cT63CKUw"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"eQ5bX4P1skWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the crawled data from the smaller languages (yue)\n!wget http://dumps.wikimedia.org/zh_yuewiki/20211020/zh_yuewiki-20211020-page.sql.gz\n!wget http://dumps.wikimedia.org/zh_yuewiki/20211020/zh_yuewiki-20211020-langlinks.sql.gz","metadata":{"id":"Dja09t57yXks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download the dataset\n!wget https://dumps.wikimedia.org/zh_yuewiki/20211020/zh_yuewiki-20211020-pages-articles.xml.bz2","metadata":{"id":"-8kfUsMg9wTJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/clab/wikipedia-parallel-titles","metadata":{"id":"5wAvT3KE3j9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#extract parallel titles\n!/content/wikipedia-parallel-titles/build-corpus.sh zh zh_yuewiki-20211020 > titles.txt","metadata":{"id":"MrwCS_Gc3S3I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#extracts and cleans text from a Wikipedia database backup dump\n!pip install wikiextractor","metadata":{"id":"4T1hMiZE4sM_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gensim","metadata":{"id":"HEaGUrDt_TvL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### YUE","metadata":{"id":"xwEbxkLCCXNX"}},{"cell_type":"code","source":"!python make_wiki_corpus.py zh_yuewiki-20211020-pages-articles.xml.bz2 wiki_yue.txt\n# 15,410 articles","metadata":{"id":"q7q7ay6r_dQz","outputId":"efec3fed-1b40-4628-a3e0-a834daa64682"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see the word count of the wiki corpus of yue\n!wc -c wiki_yue.txt | awk '{print $1}'","metadata":{"id":"58lihvACBo39","outputId":"529903cc-1f18-4b96-a024-03ef7d2f8a65"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ZH","metadata":{"id":"oqgWWOnbUtOD"}},{"cell_type":"code","source":"# get the crawled data from the Chinese language\n!wget https://dumps.wikimedia.org/zhwiki/20211020/zhwiki-20211020-pages-articles.xml.bz2","metadata":{"id":"2uj27TiqSjwW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python make_wiki_corpus.py zhwiki-20211020-pages-articles.xml.bz2 wiki_zh.txt","metadata":{"id":"RXf7seT0Ut6q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -czvf zhwiki_corpus.tar.gz wiki_zh.txt","metadata":{"id":"1RHHme0qjxZf","outputId":"06a032d0-038a-4782-eafd-35a15da1fb20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wc -c wiki_zh.txt | awk '{print $1}'","metadata":{"id":"-31as-FGUy3J","outputId":"899c9849-6ee0-48c3-8342-32f317b6835f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data loading and prepossessing ","metadata":{"id":"KabvxjVdCcQs"}},{"cell_type":"code","source":"# unzip the data file of ZH\n!tar -xvf /content/drive/MyDrive/rd_data/zhwiki_corpus.tar.gz","metadata":{"id":"6cZArp82To_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_txt(in_file):\n    with open(in_file, 'r', encoding=\"utf-8\") as f:\n       lines = [line.rstrip() for line in f]\n    return lines\n\ndef article_to_list_of_words(in_list):\n    # convert a string like variable to a list of tokens (words)\n    return list(in_list.split(\" \"))\n\ndef batch_load_lists_of_words(input_data):\n    # convert all articles to a nested list, each list containing a list of words in the file.\n    one_articles = [article_to_list_of_words(each) for each in input_data]\n    return one_articles\n\ndef fetch_titles(inlist):\n    # get all the titles in the wikipedia crawl\n    titles = [article_to_list_of_words(each)[0] for each in inlist]\n    return titles","metadata":{"id":"lGxf4FwCWDL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /content/drive/MyDrive/rd_data","metadata":{"id":"5RrthJCDhsTa","outputId":"a5f83f3d-5890-4132-b710-dfaa58ac131c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read the data files\ndatadir = \"/content/drive/MyDrive/rd_data/\"\nzh_path = datadir+\"wiki_zh.txt\"\nyue_path = datadir+\"wiki_yue.txt\"\n\nzhwiki = read_txt(\"wiki_zh.txt\")\nyuewiki = read_txt(yue_path)\n\n# load the files from lists of strings to lists of lists of words\nyue_list = batch_load_lists_of_words(yuewiki)\nzh_list = batch_load_lists_of_words(zhwiki)\nprint(yue_list[0])   #spotcheck\nprint(zh_list[0])  #spotcheck\n\n# Get a list of the titles only in both lanuuages\nyue_titles = fetch_titles(yuewiki)\nzh_titles = fetch_titles(zhwiki)\nprint(yue_titles[0])   #spotcheck\nprint(zh_titles[0])  #spotcheck","metadata":{"id":"1CXRcipWTYkt","outputId":"1f55bc9c-357e-4a94-c73b-fbde99b170eb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"There are {len(yuewiki)} articles in YUE wiki\")\nprint(f\"There are {len(zhwiki)} articles in ZH wiki\")","metadata":{"id":"R8T2w-6N5lZd","outputId":"d39acdb9-c3c5-4dcc-cb0a-588c364e5386"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install opencc","metadata":{"id":"kdqCSZoEdJqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yue_list[9222]","metadata":{"id":"diVgp6m9eY9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# need to translate the titles in ZH from SC to TC\nimport opencc\nconverter = opencc.OpenCC('s2hk.json') # Simplified to Traditional Chinese (Hong Kong variant) 簡體到香港繁體\ntranslated_ZHtitles = [converter.convert(title) for title in zh_titles]\nprint(translated_ZHtitles)\n\n# find intersection between translated_ZHtitles and yue_titles\nintersected_titles = set(translated_ZHtitles).intersection(set(yue_titles))\nprint(intersected_titles)\nprint(len(intersected_titles))","metadata":{"id":"IEX62iI3E2mk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(intersected_titles)\nprint(len(intersected_titles))","metadata":{"id":"TsIhiYSvskFS","outputId":"bbba9911-b9f7-42b2-c075-9a948418a4a8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_title_idx_in_texts(check, list_to_check):\n  positions = [\"{} {}\".format(index1,index2) for index1,value1 in enumerate(list_to_check) for index2,value2 in enumerate(value1) if value2==check]\n  nested_index_list = [list(pos.split(\" \")) for pos in positions]\n  title_index = [sublist for sublist in nested_index_list if '0' in sublist]\n  if len(title_index) < 1:\n      return None\n  flattened_title_index =  [item for sublist in title_index for item in sublist]\n  return int(flattened_title_index[0])\n\nfind_title_idx_in_texts(check=\"0\", list_to_check=yue_list)","metadata":{"id":"EKXEriD1upzT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dict(tgtstrings, tgtlist, tgtlang_code, srcstrings, srclist, srclang_code):\n    lang_dict = {}\n    # tgt lang\n    lang_dict[tgtlang_code] = {}\n    for idx, item in enumerate(tgtlist):\n        wiki_pagename = item[0]\n        if wiki_pagename in intersected_titles:\n            lang_dict[tgtlang_code][wiki_pagename] = tgtstrings[idx]\n    # src lang\n    lang_dict[srclang_code] = {}\n    for idx1, item1 in enumerate(srclist):\n        wiki_pagename_src = item1[0]\n        if wiki_pagename_src in intersected_titles:\n            lang_dict[srclang_code][wiki_pagename_src] = srcstrings[idx1]\n    return lang_dict\n  \nyuezh_dict = create_dict(yuewiki, yue_list, \"yue\", zhwiki, zh_list, \"zh\")","metadata":{"id":"rClkRBiJW_E7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(yuezh_dict.get(\"zh\", {}).get('愛因斯坦'))\nprint(yuezh_dict.get(\"yue\", {}).get('愛因斯坦'))","metadata":{"id":"1XLdTavOa_zp","outputId":"201df10d-ebd7-492d-f594-164c929046f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdd1 = pd.DataFrame.from_dict(yuezh_dict)\npdd1.head(5)","metadata":{"id":"Cl1iTOiEwUMI","outputId":"3e3a8023-0a9d-433d-98f2-a2a4ffba455b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdd1 = pdd1.dropna(subset=[\"zh\"])\npdd1 #5140 rows","metadata":{"id":"_9Z6XEtFb3MB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"converter = opencc.OpenCC('s2hk.json') # Convert texts to Traditioanl HK Chinese if any in the ZH column\n\ndef script_converter(x):\n  x1 = converter.convert(x)\n  return x1\n\npdd1[\"zh_converted\"] = pdd1[\"zh\"].apply(script_converter)","metadata":{"id":"D51Ck7GvcGtI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdd1","metadata":{"id":"NXWAo_VGdC6C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndef df_to_csv(some_df, save_as=\"zhyue_comparable_wiki.csv\"):\n    return pd.DataFrame(some_df).to_csv(save_as, index=False, encoding='utf-8')\n\ndef df_to_txt(some_df, lang=\"yue\", save_as=\"comparable_wiki.txt\"):\n    \"\"\" Save the training and test set as .txt to make a training/test set \"\"\"\n    return pd.DataFrame(some_df[lang]).to_csv(save_as,index=False, encoding='utf-8', header=False)","metadata":{"id":"n8NslQn5C9Rt","execution":{"iopub.status.busy":"2021-11-05T19:16:09.046764Z","iopub.execute_input":"2021-11-05T19:16:09.047531Z","iopub.status.idle":"2021-11-05T19:16:09.056184Z","shell.execute_reply.started":"2021-11-05T19:16:09.047489Z","shell.execute_reply":"2021-11-05T19:16:09.055396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_to_csv(pdd1, \"zhyue_comparable_wiki.csv\")\ndf_to_txt(pdd1, \"yue\", \"yue_comparable_wiki.txt\")\ndf_to_txt(pdd1, \"zh\", \"zh_comparable_wiki.txt\")","metadata":{"id":"IU0u77Atfl62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdd1.to_pickle(\"ComparableWIKICorpus-yue-zh.pkl\")\n# unpickled_df = pd.read_pickle(\"./dummy.pkl\")","metadata":{"id":"qpQFT0koC5Ef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pickle them\nimport pickle\n\nwith open('wiki_corpus.pkl', 'wb') as f:\n    pickle.dump(mylist, f)","metadata":{"id":"n06DEmDlBi4J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# load the pickled files\nwith open('../input/rd-nmt/ComparableWIKICorpus-yue-zh.pkl', 'rb') as f:\n   myDF = pickle.load(f)\n\n#myDF","metadata":{"id":"vgQJl9ZUB0t3","outputId":"4436220c-f77e-44fa-e509-23727cb35e18","execution":{"iopub.status.busy":"2021-11-05T18:58:36.291177Z","iopub.execute_input":"2021-11-05T18:58:36.291671Z","iopub.status.idle":"2021-11-05T18:58:37.299172Z","shell.execute_reply.started":"2021-11-05T18:58:36.291632Z","shell.execute_reply":"2021-11-05T18:58:37.298308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for_split_df = myDF.copy()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:58:46.024612Z","iopub.execute_input":"2021-11-05T18:58:46.024899Z","iopub.status.idle":"2021-11-05T18:58:46.031075Z","shell.execute_reply.started":"2021-11-05T18:58:46.02487Z","shell.execute_reply":"2021-11-05T18:58:46.030402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for_split_df = for_split_df.drop(\"zh\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:58:49.807923Z","iopub.execute_input":"2021-11-05T18:58:49.808571Z","iopub.status.idle":"2021-11-05T18:58:49.833888Z","shell.execute_reply.started":"2021-11-05T18:58:49.808521Z","shell.execute_reply":"2021-11-05T18:58:49.83298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(for_split_df, test_size=0.15, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T19:04:27.579304Z","iopub.execute_input":"2021-11-05T19:04:27.580051Z","iopub.status.idle":"2021-11-05T19:04:27.588654Z","shell.execute_reply.started":"2021-11-05T19:04:27.580013Z","shell.execute_reply":"2021-11-05T19:04:27.587713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train))\nprint(len(test))","metadata":{"execution":{"iopub.status.busy":"2021-11-05T19:04:29.558358Z","iopub.execute_input":"2021-11-05T19:04:29.558652Z","iopub.status.idle":"2021-11-05T19:04:29.563834Z","shell.execute_reply.started":"2021-11-05T19:04:29.558623Z","shell.execute_reply":"2021-11-05T19:04:29.562928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_t, val = train_test_split(train, test_size=0.15, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T19:04:53.218512Z","iopub.execute_input":"2021-11-05T19:04:53.219106Z","iopub.status.idle":"2021-11-05T19:04:53.226791Z","shell.execute_reply.started":"2021-11-05T19:04:53.219052Z","shell.execute_reply":"2021-11-05T19:04:53.225994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_t))\nprint(len(test))\nprint(len(val))","metadata":{"execution":{"iopub.status.busy":"2021-11-05T19:05:20.400884Z","iopub.execute_input":"2021-11-05T19:05:20.401612Z","iopub.status.idle":"2021-11-05T19:05:20.407147Z","shell.execute_reply.started":"2021-11-05T19:05:20.401571Z","shell.execute_reply":"2021-11-05T19:05:20.406344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_to_txt(train_t, \"yue\", \"CW_yue_train.txt\")\ndf_to_txt(train_t, \"zh_converted\", \"CW_zh_train.txt\")\n\ndf_to_txt(test, \"yue\", \"CW_yue_test.txt\")\ndf_to_txt(test, \"zh_converted\", \"CW_zh_test.txt\")\n\ndf_to_txt(val, \"yue\", \"CW_yue_val.txt\")\ndf_to_txt(val, \"zh_converted\", \"CW_zh_val.txt\")","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm zhwiki_corpus.tar.gz","metadata":{"execution":{"iopub.status.busy":"2021-11-05T19:22:19.141256Z","iopub.execute_input":"2021-11-05T19:22:19.141577Z","iopub.status.idle":"2021-11-05T19:22:19.910468Z","shell.execute_reply.started":"2021-11-05T19:22:19.141541Z","shell.execute_reply":"2021-11-05T19:22:19.909578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -czvf cwc_yue_zh_splited.tar.gz *.txt\n#\"CW_yue_train.txt\" \"CW_zh_train.txt\" \"CW_yue_test.txt\" \"CW_zh_test.txt\" \"CW_yue_val.txt\" \"CW_zh_val.txt\"","metadata":{"execution":{"iopub.status.busy":"2021-11-05T19:23:19.595973Z","iopub.execute_input":"2021-11-05T19:23:19.596283Z","iopub.status.idle":"2021-11-05T19:23:22.751288Z","shell.execute_reply.started":"2021-11-05T19:23:19.596249Z","shell.execute_reply":"2021-11-05T19:23:22.750465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yue_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"import os\n\ndef load_file(datapath, filename):\n    filepath = datapath + \"/\" + filename\n    file_name = filename.replace(\".\", \"_\")\n    with open(filepath, \"r\", encoding=\"utf-8\") as inf:\n        file_name = inf.read().splitlines()\n    return file_name\n\ndef load_batch(datapath, batchnames):\n    return [load_file(datapath, batchnames[i]) for i in range(len(batchnames))]\n\nfiles = os.listdir(\"../input/rd-nmt/zh-yue_data_tokenized/content/zh-yue-toks\")\nlist_of_files = [f for f in files]\n\nbatch = load_batch(\"../input/rd-nmt/zh-yue_data_tokenized/content/zh-yue-toks\", list_of_files)\n\nlist_of_names = [dataname.replace(\".\", \"_\") for dataname in list_of_files]\n\nassert len(batch) == len(list_of_files) == len(list_of_names) \nprint(list_of_names)\nprint(len(batch))\n\n# Set variable name for the each data file\nval_yue = batch[0]\ntest_yue = batch[1]\ntest_zh = batch[2]\ntrain_zh = batch[3]\ntrain_yue = batch[4]\nval_zh = batch[5]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T23:48:58.684871Z","iopub.execute_input":"2021-11-05T23:48:58.685185Z","iopub.status.idle":"2021-11-05T23:48:58.705722Z","shell.execute_reply.started":"2021-11-05T23:48:58.685151Z","shell.execute_reply":"2021-11-05T23:48:58.704553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\"\"\" Pre-requsite for MNMT models:\n    read dataset to dataframe\n\"\"\"\n\ndef df_generate(first_list, second_list, column_first=\"input_text\", column_second=\"target_text\"):\n    \"\"\" generate a dataframe from two lists \"\"\"\n    return pd.DataFrame(zip(first_list, second_list), columns = [column_first, column_second])\n    \ntrain = df_generate(train_zh, train_yue, 'input_text', 'target_text')\ntest = df_generate(test_zh, test_yue, 'input_text', 'target_text')\nval = df_generate(val_zh, val_yue, 'input_text', 'target_text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train.head(5)\ntest.head(5)\n#val.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T00:09:16.466159Z","iopub.execute_input":"2021-11-06T00:09:16.467259Z","iopub.status.idle":"2021-11-06T00:09:16.481033Z","shell.execute_reply.started":"2021-11-06T00:09:16.467208Z","shell.execute_reply":"2021-11-06T00:09:16.479775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NEW APPRAOCH: MNMT FINETUNING","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/pytorch/fairseq\n%cd fairseq\n!pip install --editable ./\n%cd ..\n\n!pip install transformers\n!pip install simpletransformers\n\n#For tokenization\n!pip install sentencepiece ","metadata":{"execution":{"iopub.status.idle":"2021-11-05T23:56:17.757124Z","shell.execute_reply.started":"2021-11-05T23:54:23.581623Z","shell.execute_reply":"2021-11-05T23:56:17.755745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training M2M model\nFairseq library is more CLI oriented rather than pythonic. To fine-tune M2M model, we need to:\n1. Download the 418M parameters model first, alongside the tokenizer and vocabulary files.\n1. Export the training and validation sentence pairs to text files.\n1. Tokenize sentences using the script under fairseq/scripts/spm_encode.py\n1. Binarize sentences for faster data loading and training.\n1. Fine-tune the model!","metadata":{}},{"cell_type":"code","source":"#Download  pretrained model, vocabulary and tokenizer\n!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/spm.128k.model\"\n!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/data_dict.128k.txt\"\n!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/model_dict.128k.txt\"\n!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/language_pairs_small_models.txt\"\n!wget \"https://dl.fbaipublicfiles.com/m2m_100/418M_last_checkpoint.pt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_txt = \"\\n\".join(train.input_text.values.tolist())\nfile = open(\"zh_txt_train.txt\", \"w\")\nfile.write(train_txt)\nfile.close()\n\n\ntrain_target_txt = \"\\n\".join(train.target_text.values.tolist())\nfile = open(\"yue_txt_train.txt\", \"w\")\nfile.write(train_target_txt)\nfile.close()\n\nvalidation_txt = \"\\n\".join(val.input_text.values.tolist())\nfile = open(\"zh_txt_validation.txt\", \"w\")\nfile.write(validation_txt)\nfile.close()\n\n\nvalidation_target_txt = \"\\n\".join(val.target_text.values.tolist())\nfile = open(\"yue_txt_validation.txt\", \"w\")\nfile.write(validation_target_txt)\nfile.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenize text\n!python fairseq/scripts/spm_encode.py \\\n        --model spm.128k.model \\\n        --output_format=piece \\\n        --inputs=zh_txt_train.txt \\\n        --outputs=train.zh\n        \n!python fairseq/scripts/spm_encode.py \\\n        --model spm.128k.model \\\n        --output_format=piece \\\n        --inputs=yue_txt_train.txt \\\n        --outputs=train.yue\n        \n!python fairseq/scripts/spm_encode.py \\\n        --model spm.128k.model \\\n        --output_format=piece \\\n        --inputs=zh_txt_validation.txt \\\n        --outputs=val.zh\n        \n!python fairseq/scripts/spm_encode.py \\\n        --model spm.128k.model \\\n        --output_format=piece \\\n        --inputs=yue_txt_validation.txt \\\n        --outputs=val.yue\n        \n#Binarize tokenized text\n!fairseq-preprocess \\\n    --source-lang zh --target-lang yue \\\n    --trainpref train \\\n    --validpref val \\\n    --thresholdsrc 0 --thresholdtgt 0 \\\n    --destdir data_bin \\\n    --srcdict model_dict.128k.txt --tgtdict model_dict.128k.txt\n    \n\n#Store checkpoints\n#!mkdir \n\n!fairseq-train data_bin \\\n  --finetune-from-model  \"./418M_last_checkpoint.pt\"\\\n  --save-dir ./ \\\n  --task translation_multi_simple_epoch \\\n  --encoder-normalize-before \\\n  --lang-pairs 'zh-yue' \\\n  --batch-size 10 \\\n  --decoder-normalize-before \\\n  --encoder-langtok src \\\n  --decoder-langtok \\\n  --criterion cross_entropy \\\n  --optimizer adafactor \\\n  --lr-scheduler cosine \\\n  --lr 3e-05 \\\n  --max-update 40000 \\\n  --update-freq 2 \\\n  --save-interval 1 \\\n  --save-interval-updates 5000 \\\n  --keep-interval-updates 10 \\\n  --no-epoch-checkpoints \\\n  --log-format simple \\\n  --log-interval 2 \\\n  --patience 10 \\\n  --arch transformer_wmt_en_de_big \\\n  --encoder-layers 12 --decoder-layers 12 \\\n  --share-decoder-input-output-embed --share-all-embeddings \\\n  --ddp-backend no_c10d \\\n  --max-epoch 10 \\\n  --wandb-project \"ZH-YUE-M2M\"","metadata":{"execution":{"iopub.status.busy":"2021-11-06T00:25:20.020203Z","iopub.execute_input":"2021-11-06T00:25:20.020593Z","iopub.status.idle":"2021-11-06T00:26:13.180559Z","shell.execute_reply.started":"2021-11-06T00:25:20.020554Z","shell.execute_reply":"2021-11-06T00:26:13.179398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate synthetic data ","metadata":{}},{"cell_type":"code","source":"!pip install numpy requests nlpaug\n!pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece\n!pip install jieba","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nlpaug.util.file.download import DownloadUtil\nDownloadUtil.download_fasttext(model_name='wiki-news-300d-1M', dest_dir='.') # Download fasttext model\n\n!pip install gensim>=4.1.2","metadata":{"execution":{"iopub.status.busy":"2021-11-06T00:47:53.336866Z","iopub.execute_input":"2021-11-06T00:47:53.338308Z","iopub.status.idle":"2021-11-06T00:48:41.614917Z","shell.execute_reply.started":"2021-11-06T00:47:53.338228Z","shell.execute_reply":"2021-11-06T00:48:41.614186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh_yue.vec","metadata":{"execution":{"iopub.status.busy":"2021-11-06T01:05:26.781779Z","iopub.execute_input":"2021-11-06T01:05:26.782174Z","iopub.status.idle":"2021-11-06T01:05:32.042632Z","shell.execute_reply.started":"2021-11-06T01:05:26.782134Z","shell.execute_reply":"2021-11-06T01:05:32.041782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Embeddings Augmenter (fasttext, Cantonese)\n\nimport jieba\n\n#jieba.enable_paddle()\n\ndef tokenizer(x):\n    return jieba.cut(text, cut_all=False)\n\ntext = '如何遇上對的人又是故事的一部份呢'\naug = naw.WordEmbsAug(model_type='fasttext', tokenizer=tokenizer,\n                      model_path=\"./wiki.zh_yue.vec\") #wiki.zh.vec\n\naugmented_text = aug.augment(text)\n\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Text:\")\nprint(augmented_text)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T01:08:34.236153Z","iopub.execute_input":"2021-11-06T01:08:34.236507Z","iopub.status.idle":"2021-11-06T01:08:34.29833Z","shell.execute_reply.started":"2021-11-06T01:08:34.236474Z","shell.execute_reply":"2021-11-06T01:08:34.296892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nlpaug.augmenter.word as naw\n\ntext = 'The quick brown fox jumped over the lazy dog'\nback_translation_aug = naw.BackTranslationAug(\n    from_model_name='facebook/wmt19-en-de', \n    to_model_name='facebook/wmt19-de-en'\n)\nback_translation_aug.augment(text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bitext mining (Mine parallel sentences)","metadata":{"id":"GbX3Z54WwHPh"}},{"cell_type":"markdown","source":"### Environment setup","metadata":{"id":"aphY0fgNzjOX"}},{"cell_type":"markdown","source":"#### One-time enviornment setup for conda","metadata":{"id":"MtRyjESfxxvg"}},{"cell_type":"code","source":"#Download and install miniconda to /content/miniconda3 directory:\n%env PYTHONPATH=\n! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh\n! chmod +x Miniconda3-py37_4.9.2-Linux-x86_64.sh\n! bash ./Miniconda3-py37_4.9.2-Linux-x86_64.sh -b -f -p /content/miniconda3","metadata":{"id":"WmCwKXmzxaOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Add miniconda to the system PATH:\nimport os\npath = '/content/miniconda3/bin:' + os.environ['PATH']\n%env PATH=$path","metadata":{"id":"Yk3qEtO4xdye","outputId":"d78e31dd-f2d2-43d8-e7bb-28d213cc991d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install faiss (similarity search) via conda\n\n!conda install -c pytorch faiss-gpu # GPU(+CPU) version","metadata":{"id":"4mb0b2n9wLlY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the version of the packagexyz and its location within the conda directory\nimport sys\n_ = sys.path.append(\"/content/miniconda3/lib/python3.7/site-packages\")\nimport faiss\nprint(faiss.__version__, faiss.__file__)","metadata":{"id":"ci7-jmfUx5UT","outputId":"dfcd40d0-7dff-446d-ad2b-4e214e2a7d2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Copy everything over to Google Drive\n!tar -zcf conda_colab.tar.gz miniconda3\n!cp conda_colab.tar.gz /content/drive/MyDrive/","metadata":{"id":"4WH__JpbyCSp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Copy conda back to Colab (run whenever restarting a notebook)","metadata":{"id":"nQRw4AeUypRM"}},{"cell_type":"code","source":"# copy back the conda installation, and re-setup the environment:\n#from google.colab import drive \n#drive.mount('/content/drive')\n\n!tar -xf /content/drive/MyDrive/conda_colab.tar.gz -C ../\n\nimport os\npath = '/content/miniconda3/bin:' + os.environ['PATH']\n%env PATH=$path\n%env PYTHONPATH=\nimport sys\n_ = sys.path.append(\"/content/miniconda3/lib/python3.7/site-packages\")","metadata":{"id":"FYPM77xzyLvJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Set up LASER","metadata":{"id":"Gj9s-XOb0Zcm"}},{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/LASER","metadata":{"id":"7rl6U1jVzsll","execution":{"iopub.status.busy":"2021-11-05T19:43:07.507346Z","iopub.execute_input":"2021-11-05T19:43:07.50779Z","iopub.status.idle":"2021-11-05T19:43:11.131469Z","shell.execute_reply.started":"2021-11-05T19:43:07.507757Z","shell.execute_reply":"2021-11-05T19:43:11.130629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set the environment variable 'LASER' to the root of the installation\nimport os\nos.environ['LASER'] = \"LASER\"","metadata":{"id":"7CFPBm7r0sPB","execution":{"iopub.status.busy":"2021-11-05T19:43:07.50135Z","iopub.execute_input":"2021-11-05T19:43:07.502134Z","iopub.status.idle":"2021-11-05T19:43:07.506003Z","shell.execute_reply.started":"2021-11-05T19:43:07.502085Z","shell.execute_reply":"2021-11-05T19:43:07.505415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!setenv  LASER /content/projects/laser","metadata":{"id":"MZ9wTCJ9yiiF","outputId":"c44aba88-08db-4f42-e461-f5c589f10f05","execution":{"iopub.status.busy":"2021-11-05T16:42:19.161212Z","iopub.execute_input":"2021-11-05T16:42:19.162048Z","iopub.status.idle":"2021-11-05T16:42:19.907396Z","shell.execute_reply.started":"2021-11-05T16:42:19.162001Z","shell.execute_reply":"2021-11-05T16:42:19.906534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export | grep LASER","metadata":{"id":"ks93stx-yJ6H","outputId":"94a46858-1e51-4ca0-e893-d2d8b223e01a","execution":{"iopub.status.busy":"2021-11-05T19:43:11.133876Z","iopub.execute_input":"2021-11-05T19:43:11.13458Z","iopub.status.idle":"2021-11-05T19:43:11.906051Z","shell.execute_reply.started":"2021-11-05T19:43:11.134526Z","shell.execute_reply":"2021-11-05T19:43:11.905212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download encoders from Amazon s3\n!bash ../input/laser_set/install_models.sh","metadata":{"id":"eTsTNKyI0k5b","execution":{"iopub.status.busy":"2021-11-05T19:29:43.392088Z","iopub.execute_input":"2021-11-05T19:29:43.392557Z","iopub.status.idle":"2021-11-05T19:29:44.167269Z","shell.execute_reply.started":"2021-11-05T19:29:43.392519Z","shell.execute_reply":"2021-11-05T19:29:44.166253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#download third party software\n!bash ./LASER/install_external_tools.sh","metadata":{"id":"4vG1F9WD1Z3l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## # Tokenize and Embed # ##\n#  calculate sentence embeddings for a text file\n# The input will be tokenized, using the mode of the specified language, \n# BPE will be applied and the sentence embeddings will be calculated.\n!bash /./LASER/source/embed.py --encoder bilstm.93langs.2018-12-26.pt --bpe-codes 93langs.fcodes --token-lang yue  --verbose --output yue_embeddings.raw","metadata":{"id":"FAgctxjM1hIa","outputId":"1cc82e20-e6a7-45fe-84c1-b67eefd1bff9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The embeddings are stored in float32 matrices in raw binary format. \n# They can be read in Python by:\nimport numpy as np\ndim = 1024\nX = np.fromfile(\"yue_embeddings.raw\", dtype=np.float32, count=-1)                                                                          \nX.resize(X.shape[0] // dim, dim) # X is a N x 1024 matrix where N is the number of lines in the text file.                                              ","metadata":{"id":"wNPns4h733xy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## a joint sentence embedding for all the considered languages","metadata":{"id":"O7QIng4e6Rsz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mine for bitexts\n\n!python /content/LASER/source/mine_bitexts.py \\\n  zh_para yue_para \\\n  --src-lang=\"zh\" --trg-lang=\"yue\" \\\n  --output mined.out --src-embeddings ${bn}.enc.${l1} --trg-embeddings ${bn}.enc.${l2} \\\n  --mode mine \\\n  --verbose #--gpu","metadata":{"id":"Mgd-kkpx7UvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/pytorch/fairseq","metadata":{"id":"7HSpbtN_uqG8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# frequency cleaning\n!wget https://dl.fbaipublicfiles.com/m2m_100/histograms.tar.gz \n!tar -xvzf histograms.tar.gz\n!python /content/fairseq/examples/m2m_100/process_data/clean_histogram.py --src \"zh\" --tgt \"yue\" --src-file zh_comparable_wiki.txt --tgt-file yue_comparable_wiki.txt  --src-output-file source_output.zh --tgt-output-file target_output.yue --histograms /content/histograms","metadata":{"id":"kQoKC-oOul4-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply SPM\n!wget https://dl.fbaipublicfiles.com/m2m_100/spm.128k.model\n!python /content/fairseq/scripts/spm_encode.py \\\n    --model spm.128k.model \\\n    --output_format=piece \\\n    --inputs=/path/to/input/file/here \\\n    --outputs=/path/to/output/file/here\n\n# length ratio cleaning\n!perl mosesdecoder/scripts/training/clean-corpus-n.perl --ratio 3 /path/to/training/data/train.spm.$src-$tgt $src $tgt /path/to/output/directory/train.spm.$src-$tgt 1 250\n\n# binarize data\n!wget https://dl.fbaipublicfiles.com/m2m_100/data_dict.128k.txt\nfairseq-preprocess \\\n    --source-lang $src --target-lang $tgt \\\n    --testpref spm.$src.$tgt \\\n    --thresholdsrc 0 --thresholdtgt 0 \\\n    --destdir data_bin \\\n    --srcdict data_dict.128k.txt --tgtdict data_dict.128k.txt","metadata":{"id":"pdbYgQVIu9XY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pretrained MNMT model from zero\n# #Ours + 12 layer + RoBT (transformer + merged attention + LaLn + LaLT)\n! wget http://data.statmt.org/bzhang/acl2020_multilingual/Ours-L12-RoBT.tar.gz \n! tar xfvz Ours-L12-RoBT.tar.gz ","metadata":{"id":"F6pLiZ0uvpGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download our preprocessed subword models\n!wget http://data.statmt.org/bzhang/acl2020_multilingual/submodels.tar.gz\n!tar xfvz submodels.tar.gz","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download the evaluation script\n!wget http://data.statmt.org/bzhang/acl2020_multilingual/example_evaluation.sh\n\n# install sacrebleu, sentencepiece if necessary\n!pip3 install sacrebleu sentencepiece --user\n# notice that we use tensorflow, so install tensorflow if necessary\n# pip install tensorflow_gpu==1.13.1 --user\n\n\n# perform decoding\nbash example_evaluation.sh","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/hangyav/UnsupPSE.git","metadata":{"execution":{"iopub.status.busy":"2021-11-06T01:17:40.48106Z","iopub.execute_input":"2021-11-06T01:17:40.481416Z","iopub.status.idle":"2021-11-06T01:17:42.262175Z","shell.execute_reply.started":"2021-11-06T01:17:40.481379Z","shell.execute_reply":"2021-11-06T01:17:42.261025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! ./UnsupPSE/get_third_party.sh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!./UnsupPSE/install_requirements.sh","metadata":{"execution":{"iopub.status.busy":"2021-11-06T01:19:44.804198Z","iopub.execute_input":"2021-11-06T01:19:44.80457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}